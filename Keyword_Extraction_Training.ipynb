{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f920a10",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edafe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keybert\n",
    "!pip install yake\n",
    "!pip install rake-nltk\n",
    "!pip install transformers\n",
    "!pip install sentence-transformers\n",
    "!pip install datasets\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install scikit-learn\n",
    "!pip install pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59268f87",
   "metadata": {},
   "source": [
    "## üìö Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Keyword Extraction Libraries\n",
    "from keybert import KeyBERT\n",
    "import yake\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Transformer Models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5d5a4",
   "metadata": {},
   "source": [
    "## üì∞ Step 3: Load News Datasets for Journalism/Research Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8330cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG News Dataset (News articles for 4 categories)\n",
    "print(\"Loading AG News Dataset...\")\n",
    "ag_news = load_dataset(\"ag_news\", split=\"train[:5000]\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_ag = pd.DataFrame({\n",
    "    'text': ag_news['text'],\n",
    "    'label': ag_news['label']\n",
    "})\n",
    "\n",
    "# Label mapping for AG News\n",
    "label_map = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
    "df_ag['category'] = df_ag['label'].map(label_map)\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"Total articles: {len(df_ag)}\")\n",
    "print(f\"\\nCategory Distribution:\")\n",
    "print(df_ag['category'].value_counts())\n",
    "\n",
    "# Display sample articles\n",
    "print(\"\\nüìù Sample Articles:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Article {i+1} ({df_ag.iloc[i]['category']}) ---\")\n",
    "    print(df_ag.iloc[i]['text'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc034ed",
   "metadata": {},
   "source": [
    "## üß† Step 4: Initialize Keyword Extraction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fed9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Initializing KeyBERT Model (Best for Semantic Keywords)...\")\n",
    "# Using all-MiniLM-L6-v2 for balance between speed and quality\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "print(\"‚úÖ KeyBERT initialized!\")\n",
    "\n",
    "print(\"\\nüîÑ Initializing YAKE Model...\")\n",
    "# YAKE parameters optimized for news articles\n",
    "yake_extractor = yake.KeywordExtractor(\n",
    "    lan=\"en\",\n",
    "    n=3,  # max ngram size\n",
    "    dedupLim=0.7,\n",
    "    dedupFunc='seqm',\n",
    "    windowsSize=1,\n",
    "    top=10,\n",
    "    features=None\n",
    ")\n",
    "print(\"‚úÖ YAKE initialized!\")\n",
    "\n",
    "print(\"\\nüîÑ Initializing RAKE Model...\")\n",
    "rake_extractor = Rake(\n",
    "    stopwords=stopwords.words('english'),\n",
    "    punctuations=None,\n",
    "    language='english',\n",
    "    max_length=3,\n",
    "    min_length=1\n",
    ")\n",
    "print(\"‚úÖ RAKE initialized!\")\n",
    "\n",
    "print(\"\\nüéâ All models ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04121f",
   "metadata": {},
   "source": [
    "## üîç Step 5: Define Keyword Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b5556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_keybert(text, top_n=10, keyphrase_ngram_range=(1, 3), \n",
    "                              use_mmr=True, diversity=0.5):\n",
    "    \"\"\"\n",
    "    Extract keywords using KeyBERT (Semantic-based)\n",
    "    Best for: Understanding context and meaning\n",
    "    \"\"\"\n",
    "    try:\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            text,\n",
    "            keyphrase_ngram_range=keyphrase_ngram_range,\n",
    "            stop_words='english',\n",
    "            use_mmr=use_mmr,\n",
    "            diversity=diversity,\n",
    "            top_n=top_n\n",
    "        )\n",
    "        return [(kw, round(score, 4)) for kw, score in keywords]\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def extract_keywords_yake(text, top_n=10):\n",
    "    \"\"\"\n",
    "    Extract keywords using YAKE (Statistical-based)\n",
    "    Best for: Fast extraction, no training needed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        keywords = yake_extractor.extract_keywords(text)\n",
    "        # YAKE returns lower scores for better keywords\n",
    "        keywords = [(kw, round(1 - score, 4)) for kw, score in keywords[:top_n]]\n",
    "        return keywords\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def extract_keywords_rake(text, top_n=10):\n",
    "    \"\"\"\n",
    "    Extract keywords using RAKE (Graph-based)\n",
    "    Best for: Multi-word keyphrases\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rake_extractor.extract_keywords_from_text(text)\n",
    "        keywords = rake_extractor.get_ranked_phrases_with_scores()[:top_n]\n",
    "        # Normalize scores\n",
    "        if keywords:\n",
    "            max_score = max(score for score, _ in keywords)\n",
    "            keywords = [(kw, round(score/max_score, 4)) for score, kw in keywords]\n",
    "        return keywords\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def extract_keywords_ensemble(text, top_n=10):\n",
    "    \"\"\"\n",
    "    Ensemble method combining all three extractors\n",
    "    Uses weighted voting for best results\n",
    "    \"\"\"\n",
    "    # Get keywords from all methods\n",
    "    keybert_kws = extract_keywords_keybert(text, top_n=15)\n",
    "    yake_kws = extract_keywords_yake(text, top_n=15)\n",
    "    rake_kws = extract_keywords_rake(text, top_n=15)\n",
    "    \n",
    "    # Combine with weights (KeyBERT has highest weight for semantic quality)\n",
    "    keyword_scores = {}\n",
    "    \n",
    "    # KeyBERT weight: 0.5\n",
    "    for kw, score in keybert_kws:\n",
    "        kw_lower = kw.lower()\n",
    "        keyword_scores[kw_lower] = keyword_scores.get(kw_lower, 0) + score * 0.5\n",
    "    \n",
    "    # YAKE weight: 0.3\n",
    "    for kw, score in yake_kws:\n",
    "        kw_lower = kw.lower()\n",
    "        keyword_scores[kw_lower] = keyword_scores.get(kw_lower, 0) + score * 0.3\n",
    "    \n",
    "    # RAKE weight: 0.2\n",
    "    for kw, score in rake_kws:\n",
    "        kw_lower = kw.lower()\n",
    "        keyword_scores[kw_lower] = keyword_scores.get(kw_lower, 0) + score * 0.2\n",
    "    \n",
    "    # Sort and return top keywords\n",
    "    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [(kw, round(score, 4)) for kw, score in sorted_keywords[:top_n]]\n",
    "\n",
    "print(\"‚úÖ All extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a9a3bd",
   "metadata": {},
   "source": [
    "## üß™ Step 6: Test Keyword Extraction on Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60564651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample news articles\n",
    "sample_texts = [\n",
    "    df_ag.iloc[0]['text'],  # World news\n",
    "    df_ag.iloc[1250]['text'],  # Sports news\n",
    "    df_ag.iloc[2500]['text'],  # Business news\n",
    "    df_ag.iloc[3750]['text'],  # Sci/Tech news\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç KEYWORD EXTRACTION COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    category = df_ag.iloc[i * 1250]['category'] if i > 0 else df_ag.iloc[0]['category']\n",
    "    print(f\"\\nüì∞ Article {i+1} - Category: {category}\")\n",
    "    print(f\"Text: {text[:200]}...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\nüîπ KeyBERT Keywords:\")\n",
    "    keybert_results = extract_keywords_keybert(text)\n",
    "    for kw, score in keybert_results[:5]:\n",
    "        print(f\"   ‚Ä¢ {kw}: {score}\")\n",
    "    \n",
    "    print(\"\\nüîπ YAKE Keywords:\")\n",
    "    yake_results = extract_keywords_yake(text)\n",
    "    for kw, score in yake_results[:5]:\n",
    "        print(f\"   ‚Ä¢ {kw}: {score}\")\n",
    "    \n",
    "    print(\"\\nüîπ RAKE Keywords:\")\n",
    "    rake_results = extract_keywords_rake(text)\n",
    "    for kw, score in rake_results[:5]:\n",
    "        print(f\"   ‚Ä¢ {kw}: {score}\")\n",
    "    \n",
    "    print(\"\\nüîπ ENSEMBLE Keywords (Best):\")\n",
    "    ensemble_results = extract_keywords_ensemble(text)\n",
    "    for kw, score in ensemble_results[:5]:\n",
    "        print(f\"   ‚Ä¢ {kw}: {score}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ba1c4",
   "metadata": {},
   "source": [
    "## üìä Step 7: Benchmark and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark on 100 articles\n",
    "test_articles = df_ag['text'].head(100).tolist()\n",
    "\n",
    "def benchmark_model(extractor_func, name, articles):\n",
    "    start_time = time.time()\n",
    "    all_keywords = []\n",
    "    for article in articles:\n",
    "        keywords = extractor_func(article)\n",
    "        all_keywords.append(keywords)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    avg_keywords = np.mean([len(kws) for kws in all_keywords])\n",
    "    return {\n",
    "        'name': name,\n",
    "        'time': round(elapsed_time, 2),\n",
    "        'avg_keywords': round(avg_keywords, 2),\n",
    "        'time_per_article': round(elapsed_time / len(articles) * 1000, 2)\n",
    "    }\n",
    "\n",
    "print(\"‚è±Ô∏è Benchmarking models on 100 articles...\\n\")\n",
    "\n",
    "results = []\n",
    "results.append(benchmark_model(extract_keywords_keybert, 'KeyBERT', test_articles))\n",
    "print(f\"‚úÖ KeyBERT: {results[-1]['time']}s\")\n",
    "\n",
    "results.append(benchmark_model(extract_keywords_yake, 'YAKE', test_articles))\n",
    "print(f\"‚úÖ YAKE: {results[-1]['time']}s\")\n",
    "\n",
    "results.append(benchmark_model(extract_keywords_rake, 'RAKE', test_articles))\n",
    "print(f\"‚úÖ RAKE: {results[-1]['time']}s\")\n",
    "\n",
    "results.append(benchmark_model(extract_keywords_ensemble, 'Ensemble', test_articles))\n",
    "print(f\"‚úÖ Ensemble: {results[-1]['time']}s\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä BENCHMARK RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "benchmark_df = pd.DataFrame(results)\n",
    "benchmark_df.columns = ['Model', 'Total Time (s)', 'Avg Keywords', 'Time/Article (ms)']\n",
    "print(benchmark_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4362f",
   "metadata": {},
   "source": [
    "## üíæ Step 8: Create Model Configuration and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory\n",
    "MODEL_DIR = 'keyword_extraction_model'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    'name': 'Keyword Extraction System',\n",
    "    'version': '1.0.0',\n",
    "    'domain': 'Journalism/Research',\n",
    "    'models': {\n",
    "        'keybert': {\n",
    "            'model_name': 'all-MiniLM-L6-v2',\n",
    "            'description': 'BERT-based semantic keyword extraction',\n",
    "            'best_for': 'Semantic understanding, context-aware keywords'\n",
    "        },\n",
    "        'yake': {\n",
    "            'language': 'en',\n",
    "            'max_ngram': 3,\n",
    "            'dedup_threshold': 0.7,\n",
    "            'description': 'Statistical keyword extraction',\n",
    "            'best_for': 'Fast extraction, no model loading'\n",
    "        },\n",
    "        'rake': {\n",
    "            'language': 'english',\n",
    "            'max_length': 3,\n",
    "            'min_length': 1,\n",
    "            'description': 'Graph-based keyword extraction',\n",
    "            'best_for': 'Multi-word keyphrases'\n",
    "        },\n",
    "        'ensemble': {\n",
    "            'weights': {'keybert': 0.5, 'yake': 0.3, 'rake': 0.2},\n",
    "            'description': 'Combined extraction using weighted voting',\n",
    "            'best_for': 'Best overall accuracy'\n",
    "        }\n",
    "    },\n",
    "    'default_params': {\n",
    "        'top_n': 10,\n",
    "        'diversity': 0.5,\n",
    "        'use_mmr': True\n",
    "    },\n",
    "    'stopwords': list(stopwords.words('english'))\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "with open(os.path.join(MODEL_DIR, 'config.json'), 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model configuration saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c18ab",
   "metadata": {},
   "source": [
    "## üîß Step 9: Create Extraction Module for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main extraction module\n",
    "extraction_module = '''\n",
    "\"\"\"\n",
    "Keyword Extraction Module\n",
    "Domain: Journalism/Research\n",
    "Models: KeyBERT, YAKE, RAKE, Ensemble\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from keybert import KeyBERT\n",
    "import yake\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "class KeywordExtractor:\n",
    "    def __init__(self, config_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the Keyword Extractor with all models\n",
    "        \"\"\"\n",
    "        # Load configuration\n",
    "        if config_path and os.path.exists(config_path):\n",
    "            with open(config_path, 'r') as f:\n",
    "                self.config = json.load(f)\n",
    "        else:\n",
    "            self.config = self._default_config()\n",
    "        \n",
    "        # Initialize models\n",
    "        self._init_models()\n",
    "    \n",
    "    def _default_config(self):\n",
    "        return {\n",
    "            'models': {\n",
    "                'keybert': {'model_name': 'all-MiniLM-L6-v2'},\n",
    "                'yake': {'language': 'en', 'max_ngram': 3, 'dedup_threshold': 0.7},\n",
    "                'rake': {'language': 'english', 'max_length': 3, 'min_length': 1},\n",
    "                'ensemble': {'weights': {'keybert': 0.5, 'yake': 0.3, 'rake': 0.2}}\n",
    "            },\n",
    "            'default_params': {'top_n': 10, 'diversity': 0.5, 'use_mmr': True}\n",
    "        }\n",
    "    \n",
    "    def _init_models(self):\n",
    "        \"\"\"Initialize all keyword extraction models\"\"\"\n",
    "        # KeyBERT\n",
    "        model_name = self.config['models']['keybert']['model_name']\n",
    "        self.keybert_model = KeyBERT(model=model_name)\n",
    "        \n",
    "        # YAKE\n",
    "        yake_config = self.config['models']['yake']\n",
    "        self.yake_model = yake.KeywordExtractor(\n",
    "            lan=yake_config['language'],\n",
    "            n=yake_config['max_ngram'],\n",
    "            dedupLim=yake_config['dedup_threshold'],\n",
    "            dedupFunc='seqm',\n",
    "            windowsSize=1,\n",
    "            top=20\n",
    "        )\n",
    "        \n",
    "        # RAKE\n",
    "        rake_config = self.config['models']['rake']\n",
    "        self.rake_model = Rake(\n",
    "            stopwords=stopwords.words('english'),\n",
    "            language=rake_config['language'],\n",
    "            max_length=rake_config['max_length'],\n",
    "            min_length=rake_config['min_length']\n",
    "        )\n",
    "    \n",
    "    def extract_keybert(self, text, top_n=10, diversity=0.5):\n",
    "        \"\"\"Extract keywords using KeyBERT\"\"\"\n",
    "        try:\n",
    "            keywords = self.keybert_model.extract_keywords(\n",
    "                text,\n",
    "                keyphrase_ngram_range=(1, 3),\n",
    "                stop_words='english',\n",
    "                use_mmr=True,\n",
    "                diversity=diversity,\n",
    "                top_n=top_n\n",
    "            )\n",
    "            return [(kw, round(score, 4)) for kw, score in keywords]\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def extract_yake(self, text, top_n=10):\n",
    "        \"\"\"Extract keywords using YAKE\"\"\"\n",
    "        try:\n",
    "            keywords = self.yake_model.extract_keywords(text)\n",
    "            return [(kw, round(1 - score, 4)) for kw, score in keywords[:top_n]]\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def extract_rake(self, text, top_n=10):\n",
    "        \"\"\"Extract keywords using RAKE\"\"\"\n",
    "        try:\n",
    "            self.rake_model.extract_keywords_from_text(text)\n",
    "            keywords = self.rake_model.get_ranked_phrases_with_scores()[:top_n]\n",
    "            if keywords:\n",
    "                max_score = max(score for score, _ in keywords)\n",
    "                return [(kw, round(score/max_score, 4)) for score, kw in keywords]\n",
    "            return []\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def extract_ensemble(self, text, top_n=10):\n",
    "        \"\"\"Extract keywords using ensemble method\"\"\"\n",
    "        weights = self.config['models']['ensemble']['weights']\n",
    "        \n",
    "        keybert_kws = self.extract_keybert(text, top_n=15)\n",
    "        yake_kws = self.extract_yake(text, top_n=15)\n",
    "        rake_kws = self.extract_rake(text, top_n=15)\n",
    "        \n",
    "        keyword_scores = {}\n",
    "        \n",
    "        for kw, score in keybert_kws:\n",
    "            kw_lower = kw.lower()\n",
    "            keyword_scores[kw_lower] = keyword_scores.get(kw_lower, 0) + score * weights['keybert']\n",
    "        \n",
    "        for kw, score in yake_kws:\n",
    "            kw_lower = kw.lower()\n",
    "            keyword_scores[kw_lower] = keyword_scores.get(kw_lower, 0) + score * weights['yake']\n",
    "        \n",
    "        for kw, score in rake_kws:\n",
    "            kw_lower = kw.lower()\n",
    "            keyword_scores[kw_lower] = keyword_scores.get(kw_lower, 0) + score * weights['rake']\n",
    "        \n",
    "        sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [(kw, round(score, 4)) for kw, score in sorted_keywords[:top_n]]\n",
    "    \n",
    "    def extract(self, text, method='ensemble', top_n=10, **kwargs):\n",
    "        \"\"\"\n",
    "        Main extraction method\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to extract keywords from\n",
    "            method: 'keybert', 'yake', 'rake', or 'ensemble' (default)\n",
    "            top_n: Number of keywords to return\n",
    "        \n",
    "        Returns:\n",
    "            List of (keyword, score) tuples\n",
    "        \"\"\"\n",
    "        methods = {\n",
    "            'keybert': self.extract_keybert,\n",
    "            'yake': self.extract_yake,\n",
    "            'rake': self.extract_rake,\n",
    "            'ensemble': self.extract_ensemble\n",
    "        }\n",
    "        \n",
    "        extractor = methods.get(method.lower(), self.extract_ensemble)\n",
    "        return extractor(text, top_n=top_n, **kwargs)\n",
    "    \n",
    "    def extract_all(self, text, top_n=10):\n",
    "        \"\"\"\n",
    "        Extract keywords using all methods\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with results from all methods\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'keybert': self.extract_keybert(text, top_n),\n",
    "            'yake': self.extract_yake(text, top_n),\n",
    "            'rake': self.extract_rake(text, top_n),\n",
    "            'ensemble': self.extract_ensemble(text, top_n)\n",
    "        }\n",
    "'''\n",
    "\n",
    "# Save the extraction module\n",
    "with open(os.path.join(MODEL_DIR, 'extractor.py'), 'w') as f:\n",
    "    f.write(extraction_module)\n",
    "\n",
    "print(\"‚úÖ Extraction module saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05729254",
   "metadata": {},
   "source": [
    "## üì¶ Step 10: Create Requirements File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = '''\n",
    "# Keyword Extraction Requirements\n",
    "keybert>=0.7.0\n",
    "yake>=0.4.8\n",
    "rake-nltk>=1.0.6\n",
    "sentence-transformers>=2.2.0\n",
    "transformers>=4.25.0\n",
    "torch>=1.13.0\n",
    "nltk>=3.8.0\n",
    "scikit-learn>=1.2.0\n",
    "numpy>=1.23.0\n",
    "pandas>=1.5.0\n",
    "flask>=2.2.0\n",
    "flask-cors>=3.0.10\n",
    "gunicorn>=20.1.0\n",
    "'''\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'requirements.txt'), 'w') as f:\n",
    "    f.write(requirements.strip())\n",
    "\n",
    "print(\"‚úÖ Requirements file saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee8fb9",
   "metadata": {},
   "source": [
    "## üóúÔ∏è Step 11: Create ZIP File for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b401923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ZIP file containing all model files\n",
    "ZIP_NAME = 'keyword_extraction_model.zip'\n",
    "\n",
    "with zipfile.ZipFile(ZIP_NAME, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(MODEL_DIR):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, MODEL_DIR)\n",
    "            zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"‚úÖ ZIP file created: {ZIP_NAME}\")\n",
    "print(f\"üì¶ File size: {os.path.getsize(ZIP_NAME) / 1024:.2f} KB\")\n",
    "\n",
    "# List contents\n",
    "print(\"\\nüìã ZIP Contents:\")\n",
    "with zipfile.ZipFile(ZIP_NAME, 'r') as zipf:\n",
    "    for name in zipf.namelist():\n",
    "        print(f\"   ‚Ä¢ {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa5975",
   "metadata": {},
   "source": [
    "## ‚¨áÔ∏è Step 12: Download the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ZIP file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading keyword_extraction_model.zip...\")\n",
    "files.download(ZIP_NAME)\n",
    "print(\"\\n‚úÖ Download complete!\")\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(\"1. Extract the ZIP file\")\n",
    "print(\"2. Place the 'model' folder in your Flask backend directory\")\n",
    "print(\"3. Run the Flask server\")\n",
    "print(\"4. Use the frontend to extract keywords!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72abcad8",
   "metadata": {},
   "source": [
    "## üß™ Step 13: Final Test - Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10769953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete pipeline with a journalism sample\n",
    "sample_journalism_text = \"\"\"\n",
    "The Federal Reserve announced a significant interest rate hike on Wednesday, \n",
    "marking the fourth consecutive increase this year as the central bank continues \n",
    "its aggressive campaign to combat inflation. Fed Chair Jerome Powell stated that \n",
    "the decision was necessary to bring inflation back to the 2% target, despite \n",
    "concerns about potential economic slowdown. Wall Street reacted with mixed \n",
    "sentiments, with the Dow Jones Industrial Average initially dropping 300 points \n",
    "before recovering some losses. Economists predict that the housing market and \n",
    "consumer spending will be most affected by the rate changes. The unemployment \n",
    "rate remains historically low at 3.5%, providing some cushion against recession fears.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üì∞ FINAL PIPELINE TEST - JOURNALISM ARTICLE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìù Sample Text:\\n{sample_journalism_text}\")\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "\n",
    "# Extract keywords using all methods\n",
    "print(\"\\nüîë EXTRACTED KEYWORDS:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ KeyBERT (Semantic-based):\")\n",
    "for kw, score in extract_keywords_keybert(sample_journalism_text):\n",
    "    print(f\"   ‚ñ™ {kw} ({score})\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ YAKE (Statistical-based):\")\n",
    "for kw, score in extract_keywords_yake(sample_journalism_text):\n",
    "    print(f\"   ‚ñ™ {kw} ({score})\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ RAKE (Graph-based):\")\n",
    "for kw, score in extract_keywords_rake(sample_journalism_text):\n",
    "    print(f\"   ‚ñ™ {kw} ({score})\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ ENSEMBLE (Best Combined):\")\n",
    "for kw, score in extract_keywords_ensemble(sample_journalism_text):\n",
    "    print(f\"   ‚òÖ {kw} ({score})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ PIPELINE TEST COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f2e4f",
   "metadata": {},
   "source": [
    "## üìñ Documentation & Usage\n",
    "\n",
    "### Model Summary\n",
    "\n",
    "| Model | Type | Best For | Speed |\n",
    "|-------|------|----------|-------|\n",
    "| **KeyBERT** | Semantic (BERT) | Context-aware extraction | Medium |\n",
    "| **YAKE** | Statistical | Fast extraction | Fast |\n",
    "| **RAKE** | Graph-based | Multi-word phrases | Fast |\n",
    "| **Ensemble** | Combined | Best accuracy | Slower |\n",
    "\n",
    "### Dataset Reference\n",
    "- **AG News**: 120K news articles from 4 categories (World, Sports, Business, Sci/Tech)\n",
    "- **Source**: https://huggingface.co/datasets/ag_news\n",
    "\n",
    "### Usage Example\n",
    "```python\n",
    "from extractor import KeywordExtractor\n",
    "\n",
    "# Initialize\n",
    "extractor = KeywordExtractor(config_path='config.json')\n",
    "\n",
    "# Extract keywords\n",
    "text = \"Your news article text here...\"\n",
    "keywords = extractor.extract(text, method='ensemble', top_n=10)\n",
    "\n",
    "for keyword, score in keywords:\n",
    "    print(f\"{keyword}: {score}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
